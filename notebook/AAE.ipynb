{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import lightgbm as lgb\n",
    "\n",
    "class UJIDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, train = True, transform = None, target_transform = None, download = False):\n",
    "        self.root = root\n",
    "        dir_path = self.root + '/UJIndoorLoc'\n",
    "        zip_path = self.root + '/uji_uil.zip'\n",
    "        dataset_training_file = dir_path + '/trainingData.csv'\n",
    "        dataset_validation_file = dir_path + '/validationData.csv'\n",
    "        # Load independent variables (WAPs values)\n",
    "        if train:\n",
    "            dataset_file = dataset_training_file\n",
    "        else:\n",
    "            dataset_file = dataset_validation_file\n",
    "        file = open(dataset_file, 'r')\n",
    "        # Load labels\n",
    "        label = file.readline()\n",
    "        label = label.split(',')\n",
    "        # Load independent variables\n",
    "        file_load = np.loadtxt(file, delimiter = ',', skiprows = 1)\n",
    "        #file_load_label = np.loadtxt(file, delimiter = ',')\n",
    "        #data = np.genfromtxt(file, dtype = float, delimiter = ',', names = True)\n",
    "        # RSSI values\n",
    "        self.x = file_load[:, 0 : 520]\n",
    "        # Load dependent variables\n",
    "        self.y = file_load[:, 520 : 524]\n",
    "        # Divide labels into x and y\n",
    "        self.x_label = label[0 : 520]\n",
    "        self.x_label = np.concatenate([self.x_label, label[524: 529]])\n",
    "        self.y_label = label[520 : 524]\n",
    "        # Regularization of independent variables\n",
    "        self.x[self.x == 100] = np.nan    # WAP not detected\n",
    "        self.x = self.x + 104             # Convert into positive values\n",
    "        self.x = self.x / 104             # Regularize into scale between 0 and 1\n",
    "        # Building ID, Space ID, Relative Position, User ID, Phone ID and Timestamp respectively\n",
    "        self.x = np.concatenate([self.x, file_load[:, 524 : 529]], axis = 1)\n",
    "        file.close()\n",
    "        # Reduce the number of dependent variables by combining building number and floor into one variable: area\n",
    "        area = self.y[:, 3] * 5 + self.y[:, 2]\n",
    "        self.y = np.column_stack((self.y, area))\n",
    "    def to_tensor(self):\n",
    "        self.x = torch.from_numpy(self.x).float()\n",
    "        self.y = torch.from_numpy(self.y).float()\n",
    "        self.area = torch.from_numpy(self.area).float()\n",
    "    def nan_to_zero(self):\n",
    "        self.x = np.nan_to_num(self.x)\n",
    "    # Return the target instance (row)\n",
    "    def __getitem__(self, index_row):\n",
    "        return self.x[index_row, :], self.y[index_row, :]\n",
    "    # Return the number of instances (the number of rows)\n",
    "    def __len__(self, dim = 0):\n",
    "        return int(self.x.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Euclidean distance (unit: meter) between two coordinates in EPSG:3857 \n",
    "def euclidean_distance(latitude_1, longitude_1, latitude_2, longitude_2):\n",
    "    return np.sqrt((latitude_1 - latitude_2)**2 + (longitude_1 - longitude_2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataset with lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0., 0., 1., 1.],\n",
      "        [0., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros([2,2])\n",
    "print(a)\n",
    "b = torch.ones([2,2])\n",
    "c = torch.cat((a, b), dim = 1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training dataset\n",
    "dataset_train = UJIDataset('./data', train = True)\n",
    "dataset_test = UJIDataset('./data', train = False)\n",
    "#gb_train_data = lgb.Dataset("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_train.nan_to_zero()\n",
    "#dataset_validate.nan_to_zero()\n",
    "#dataset_train.to_tensor()\n",
    "#dataset_validate.to_tensor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.53662120e+03  4.86493423e+06  2.00000000e+00  1.00000000e+00\n",
      "   7.00000000e+00]\n",
      " [-7.51915240e+03  4.86494953e+06  2.00000000e+00  1.00000000e+00\n",
      "   7.00000000e+00]\n",
      " [-7.52457040e+03  4.86493409e+06  2.00000000e+00  1.00000000e+00\n",
      "   7.00000000e+00]\n",
      " ...\n",
      " [-7.51684150e+03  4.86488929e+06  3.00000000e+00  1.00000000e+00\n",
      "   8.00000000e+00]\n",
      " [-7.53732190e+03  4.86489578e+06  3.00000000e+00  1.00000000e+00\n",
      "   8.00000000e+00]\n",
      " [-7.53616580e+03  4.86489786e+06  3.00000000e+00  1.00000000e+00\n",
      "   8.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_train.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_validate, y_train, y_validate = sklearn.model_selection.train_test_split(dataset_train.x, dataset_train.y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.68013460e+03  4.86493149e+06  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]\n",
      " [-7.37280290e+03  4.86484930e+06  4.00000000e+00  2.00000000e+00\n",
      "   1.40000000e+01]\n",
      " [-7.37707827e+03  4.86484296e+06  0.00000000e+00  2.00000000e+00\n",
      "   1.00000000e+01]\n",
      " ...\n",
      " [-7.40199620e+03  4.86479002e+06  0.00000000e+00  2.00000000e+00\n",
      "   1.00000000e+01]\n",
      " [-7.36838610e+03  4.86476941e+06  3.00000000e+00  2.00000000e+00\n",
      "   1.30000000e+01]\n",
      " [-7.64097620e+03  4.86501066e+06  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_reg_long_lgb = lgb.Dataset(x_train, label = y_train[:, 0])\n",
    "dataset_validate_reg_long_lgb = lgb.Dataset(x_validate, label = y_validate[:, 0])\n",
    "dataset_train_reg_lat_lgb = lgb.Dataset(x_train, label = y_train[:, 1])\n",
    "dataset_validate_reg_lat_lgb = lgb.Dataset(x_validate, label = y_validate[:, 1])\n",
    "dataset_train_cat_floor_lgb = lgb.Dataset(x_train, label = y_train[:, 2])\n",
    "dataset_validate_cat_floor_lgb = lgb.Dataset(x_validate, label = y_validate[:, 2])\n",
    "dataset_train_cat_building_lgb = lgb.Dataset(x_train, label = y_train[:, 3])\n",
    "dataset_validate_cat_building_lgb = lgb.Dataset(x_validate, label = y_validate[:, 3])\n",
    "dataset_train_cat_area_lgb = lgb.Dataset(x_train, label = y_train[:, 4])\n",
    "dataset_validate_cat_area_lgb = lgb.Dataset(x_validate, label = y_validate[:, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_reg = {'learning_rate': 0.001,\n",
    "              'num_boost_round': 1000,\n",
    "              'max_depth': -1,\n",
    "              'boosting': 'rf', \n",
    "              'objective': 'regression', \n",
    "              'metric': 'rmse', \n",
    "              #'is_training_metric': True, \n",
    "              #'num_leaves': 144,  \n",
    "              #'feature_fraction': 0.9, \n",
    "              'bagging_fraction': 0.9, \n",
    "              'bagging_freq': 5,\n",
    "              #'early_stopping_round': 100,\n",
    "              #'seed':2018\n",
    "             }\n",
    "params_cat_building = {'learning_rate': 0.002,\n",
    "              'num_boost_round': 500,\n",
    "              'max_depth': 16,\n",
    "              'boosting': 'rf',\n",
    "              'objective': 'multiclass',\n",
    "              'num_class': 3,\n",
    "              'metric': 'multi_logloss',\n",
    "              #'is_training_metric': True,\n",
    "              'num_leaves': 144,\n",
    "              #'feature_fraction': 0.97,\n",
    "              'bagging_fraction': 0.7,\n",
    "              'bagging_freq': 5,\n",
    "              #'early_stopping_round': 100,\n",
    "             }\n",
    "params_cat_floor = {'learning_rate': 0.002,\n",
    "              'num_boost_round': 500,\n",
    "              'max_depth': 16,\n",
    "              'boosting': 'rf',\n",
    "              'objective': 'multiclass',\n",
    "              'num_class': 5,\n",
    "              'metric': 'multi_logloss',\n",
    "              #'is_training_metric': True,\n",
    "              'num_leaves': 144,\n",
    "              #'feature_fraction': 0.97,\n",
    "              'bagging_fraction': 0.7,\n",
    "              'bagging_freq': 5,\n",
    "              #'early_stopping_round': 100,\n",
    "             }\n",
    "params_cat_area = {'learning_rate': 0.002,\n",
    "              'num_boost_round': 500,\n",
    "              'max_depth': 16,\n",
    "              'boosting': 'rf',\n",
    "              'objective': 'multiclass',\n",
    "              'num_class': 15,\n",
    "              'metric': 'multi_logloss',\n",
    "              #'is_training_metric': True,\n",
    "              'num_leaves': 144,\n",
    "              #'feature_fraction': 0.97,\n",
    "              'bagging_fraction': 0.7,\n",
    "              'bagging_freq': 5,\n",
    "              #'early_stopping_round': 100,\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feval(preds: array, train_data: Dataset) -> name: string, eval_result: float, is_higher_better: bool\n",
    "#def multiclass_score(preds, train_data):\n",
    "#    label = train_data.get_label()\n",
    "#    return 'multiclass_score', np.mean(label == preds), True\n",
    "\n",
    "#def regression_score(preds, train_data):\n",
    "#    label = train_data.get_label()\n",
    "#    return 'regression_score', np.mean(euclidean_distance(label, 0, preds, 0)), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg_long_lgb = lgb.train(params_reg, dataset_train_reg_long_lgb, dataset_validate_reg_long_lgb, verbose_eval = 100)# early_stopping_rounds = 100)\n",
    "model_reg_lat_lgb = lgb.train(params_reg, dataset_train_reg_lat_lgb, dataset_validate_reg_lat_lgb, verbose_eval = 100)#, early_stopping_rounds = 100)\n",
    "model_cat_floor_lgb = lgb.train(params_cat_floor, dataset_train_cat_floor_lgb, dataset_validate_cat_floor_lgb, verbose_eval = 100)#, early_stopping_rounds = 100)\n",
    "model_cat_building_lgb = lgb.train(params_cat_building, dataset_train_cat_building_lgb, dataset_validate_cat_building_lgb, verbose_eval = 100)#, early_stopping_rounds = 100)\n",
    "model_cat_area_lgb = lgb.train(params_cat_area, dataset_train_cat_area_lgb, dataset_validate_cat_area_lgb, verbose_eval = 100)#, early_stopping_rounds = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_long_train = model_reg_long_lgb.predict(x_train)\n",
    "predict_long_validate = model_reg_long_lgb.predict(x_validate)\n",
    "predict_lat_train = model_reg_lat_lgb.predict(x_train)\n",
    "predict_lat_validate = model_reg_lat_lgb.predict(x_validate)\n",
    "predict_floor_train = model_cat_floor_lgb.predict(x_train)\n",
    "predict_floor_validate = model_cat_floor_lgb.predict(x_validate)\n",
    "predict_building_train = model_cat_building_lgb.predict(x_train)\n",
    "predict_building_validate = model_cat_building_lgb.predict(x_validate)\n",
    "predict_area_train = model_cat_area_lgb.predict(x_train)\n",
    "predict_area_validate = model_cat_area_lgb.predict(x_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_long_test = model_reg_long_lgb.predict(dataset_test.x)\n",
    "#predict_lat_test = model_reg_lat_lgb.predict(dataset_test.x)\n",
    "#predict_floor_test = model_cat_floor_lgb.predict(dataset_test.x)\n",
    "#predict_building_test = model_cat_building_lgb.predict(dataset_test.x)\n",
    "#predict_area_test = model_cat_area_lgb.predict(dataset_test.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mse_long = sklearn.metrics.mean_squared_error(y_validate[:, 0], predict_long_test)\n",
    "#mse_lat = sklearn.metrics.mean_squared_error(y_validate[:, 1], predict_lat_test)\n",
    "#mse_floor = sklearn.metrics.mean_squared_error(y_validate[:, 2], predict_floor_test)\n",
    "#mse_building = sklearn.metrics.mean_squared_error(y_validate[:, 3], predict_building_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4864931.4921   4864849.3028   4864842.961439 ... 4864790.0184\n",
      " 4864769.4062   4865010.6595  ]\n",
      "[4864932.74729769 4864836.59820129 4864835.67615176 ... 4864843.17548275\n",
      " 4864777.75223911 4864997.52198648]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:, 1])\n",
    "print(predict_lat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7680.1346     -7372.8029     -7377.07826999 ... -7401.9962\n",
      " -7368.3861     -7640.9762    ]\n",
      "[-7650.22033727 -7373.9548471  -7371.8573325  ... -7396.09189039\n",
      " -7355.18673302 -7637.50597817]\n",
      "[-7511.5215     -7535.39365217 -7348.8982     ... -7408.69525072\n",
      " -7445.55787384 -7390.7612    ]\n",
      "[-7513.98022784 -7527.57676579 -7351.12691394 ... -7425.06455527\n",
      " -7425.06455527 -7373.70904444]\n",
      "[0. 4. 0. ... 0. 3. 0.]\n",
      "[0 4 0 ... 0 3 0]\n",
      "[2. 2. 3. ... 2. 0. 3.]\n",
      "[2 2 3 ... 2 0 3]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:, 0])\n",
    "print(predict_long_train)\n",
    "print(y_validate[:, 0])\n",
    "print(predict_long_validate)\n",
    "print(y_train[:, 2])\n",
    "print(predict_floor_train.argmax(axis = 1))\n",
    "print(y_validate[:, 2])\n",
    "print(predict_floor_validate.argmax(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3988\n"
     ]
    }
   ],
   "source": [
    "print(len(predict_floor_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_building_train = []\n",
    "error_building_validate = []\n",
    "error_floor_train = []\n",
    "error_floor_validate = []\n",
    "error_area_train = []\n",
    "error_area_validate = []\n",
    "predict_floor_argmax_train = predict_floor_train.argmax(axis = 1)\n",
    "predict_floor_argmax_validate = predict_floor_validate.argmax(axis = 1)\n",
    "predict_building_argmax_train = predict_building_train.argmax(axis = 1)\n",
    "predict_building_argmax_validate = predict_building_validate.argmax(axis = 1)\n",
    "predict_area_argmax_train = predict_area_train.argmax(axis = 1)\n",
    "predict_area_argmax_validate = predict_area_validate.argmax(axis = 1)\n",
    "for i in range(len(predict_floor_train)):\n",
    "    if predict_floor_argmax_train[i] != y_train[i, 2]:\n",
    "        error_floor_train.append(i)\n",
    "    if predict_building_argmax_train[i] != y_train[i, 3]:\n",
    "        error_building_train.append(i)\n",
    "    if predict_area_argmax_train[i] != y_train[i, 4]:\n",
    "        error_area_train.append(i)\n",
    "for i in range(len(predict_floor_validate)):\n",
    "    if predict_floor_argmax_validate[i] != y_validate[i, 2]:\n",
    "        error_floor_validate.append(i)\n",
    "    if predict_building_argmax_validate[i] != y_validate[i, 3]:\n",
    "        error_building_validate.append(i)\n",
    "    if predict_area_argmax_validate[i] != y_validate[i, 4]:\n",
    "        error_area_validate.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_rate_floor_train 0.014609982442939554\n",
      "error_rate_floor_validate 0.004201153749686481\n",
      "error_rate_building_train 0.0008151492350137948\n",
      "error_rate_buildilng_validate 0.0006270378730875344\n",
      "error_rate_area_train 0.0218209179834462\n",
      "error_rate_area_validate 0.03009027081243731\n"
     ]
    }
   ],
   "source": [
    "error_rate_floor_train = len(error_floor_train) / len(predict_floor_train)\n",
    "print('error_rate_floor_train', error_rate_floor_train)\n",
    "error_rate_floor_validate = len(error_floor_validate) / len(predict_floor_train)\n",
    "print('error_rate_floor_validate', error_rate_floor_validate)\n",
    "error_rate_building_train = len(error_building_train) / len(predict_building_train)\n",
    "print('error_rate_building_train', error_rate_building_train)\n",
    "error_rate_building_validate = len(error_building_validate) / len(predict_building_train)\n",
    "print('error_rate_buildilng_validate', error_rate_building_validate)\n",
    "error_rate_area_train = len(error_area_train) / len(predict_area_train)\n",
    "print('error_rate_area_train', error_rate_area_train)\n",
    "error_rate_area_validate = len(error_area_validate) / len(predict_area_validate)\n",
    "print('error_rate_area_validate', error_rate_area_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15948\n"
     ]
    }
   ],
   "source": [
    "print(len(predict_long_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error_mean_distance_train:  14.419452822140073\n",
      "error_max_distance_train:  159.48982321538642\n",
      "error_min_distance_train:  0.33754468490026307\n",
      "error_std_distance_train:  10.925451303028265\n",
      "error_var_distance_train:  119.365486174842\n",
      "error_mean_distance_validate:  14.64772223117856\n",
      "error_max_distance_validate:  159.48982321538642\n",
      "error_min_distance_validate:  0.37801860729130154\n",
      "error_std_distance_validate:  11.413470507630771\n",
      "error_var_distance_validate:  130.2673090285574\n"
     ]
    }
   ],
   "source": [
    "error_distance_train = []\n",
    "error_distance_validate = []\n",
    "for i in range(len(predict_long_train)):\n",
    "    error_distance_train.append(euclidean_distance(predict_lat_train[i], predict_long_train[i], y_train[i, 1], y_train[i, 0]))\n",
    "for i in range(len(predict_long_validate)):\n",
    "    error_distance_validate.append(euclidean_distance(predict_lat_validate[i], predict_long_validate[i], y_validate[i, 1], y_validate[i, 0]))\n",
    "error_mean_distance_train = np.mean(np.stack(error_distance_train)).item()\n",
    "error_max_distance_train = np.max(np.stack(error_distance_train)).item()\n",
    "error_min_distance_train = np.min(np.stack(error_distance_train)).item()\n",
    "error_std_distance_train = np.std(np.stack(error_distance_train)).item()\n",
    "error_var_distance_train = np.var(np.stack(error_distance_train)).item()\n",
    "print('error_mean_distance_train: ', error_mean_distance_train)\n",
    "print('error_max_distance_train: ', error_max_distance_train)\n",
    "print('error_min_distance_train: ', error_min_distance_train)\n",
    "print('error_std_distance_train: ', error_std_distance_train)\n",
    "print('error_var_distance_train: ', error_var_distance_train)\n",
    "error_mean_distance_validate = np.mean(np.stack(error_distance_validate)).item()\n",
    "error_max_distance_validate = np.max(np.stack(error_distance_validate)).item()\n",
    "error_min_distance_validate = np.min(np.stack(error_distance_validate)).item()\n",
    "error_std_distance_validate = np.std(np.stack(error_distance_validate)).item()\n",
    "error_var_distance_validate = np.var(np.stack(error_distance_validate)).item()\n",
    "print('error_mean_distance_validate: ', error_mean_distance_validate)\n",
    "print('error_max_distance_validate: ', error_max_distance_validate)\n",
    "print('error_min_distance_validate: ', error_min_distance_validate)\n",
    "print('error_std_distance_validate: ', error_std_distance_validate)\n",
    "print('error_var_distance_validate: ', error_var_distance_validate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
